{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CamembertConfig, CamembertModel, AutoTokenizer, CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 20 minutes articles\n",
    "df_articles = pd.read_parquet('../newspaper_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arts-stars</th>\n",
       "      <td>28294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economie</th>\n",
       "      <td>41992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high-tech</th>\n",
       "      <td>13565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planete</th>\n",
       "      <td>11055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politique</th>\n",
       "      <td>16601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>71847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title\n",
       "category_id       \n",
       "arts-stars   28294\n",
       "economie     41992\n",
       "high-tech    13565\n",
       "planete      11055\n",
       "politique    16601\n",
       "sport        71847"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = df_articles.sort_values(\"article_date\")\n",
    "df_autres = df_articles[(df_articles.category_id != 'planete') & (df_articles.category_id != 'sport') & (df_articles.category_id != 'economie') & (df_articles.category_id != 'arts-stars') & (df_articles.category_id != 'high-tech') & (df_articles.category_id != 'politique')]\n",
    "df_autres = df_autres[(df_autres.body != '') & (df_autres.title != '')]\n",
    "df_autres[\"category_id\"] = \"autres\"\n",
    "df_articles = df_articles[(df_articles.category_id == 'planete') | (df_articles.category_id == 'sport')| (df_articles.category_id == 'economie')| (df_articles.category_id == 'arts-stars')| (df_articles.category_id == 'high-tech')| (df_articles.category_id == 'politique')]\n",
    "df_articles = df_articles[(df_articles.body != '') & (df_articles.title != '')]\n",
    "df_articles.groupby(\"category_id\").count()[['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_in_parts(txt):\n",
    "    \"\"\"\n",
    "    Split articles in part of length 500 max to be compatible with camembert model.\n",
    "    \"\"\"\n",
    "    n = len(txt)\n",
    "    prev_cursor = 0\n",
    "    cursor = min(499, n-1)\n",
    "    parts = []\n",
    "    while prev_cursor < n-1:\n",
    "        while '.' not in txt[cursor] and cursor > prev_cursor:\n",
    "            cursor -= 1\n",
    "        if cursor == prev_cursor:\n",
    "            parts.append(txt[prev_cursor:min(prev_cursor+500, n)])\n",
    "            prev_cursor = min(prev_cursor+500, n)\n",
    "            cursor = min(prev_cursor+499, n-1)\n",
    "        else:\n",
    "            parts.append(txt[prev_cursor:cursor+1])\n",
    "            prev_cursor = cursor+1\n",
    "            cursor = prev_cursor+499\n",
    "            if cursor >= n-1 and prev_cursor < n-1:\n",
    "                parts.append(txt[prev_cursor:])\n",
    "                break\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {'planete': 0, 'sport': 1, 'economie': 2, 'arts-stars': 3, 'high-tech': 4, 'politique': 5, 'autres': 6}\n",
    "p_train, p_test = df_articles[(df_articles.category_id == 'planete')].iloc[:4000],  df_articles[(df_articles.category_id == 'planete')].iloc[7000:8000]\n",
    "s_train, s_test = df_articles[(df_articles.category_id == 'sport')].iloc[:4000],  df_articles[(df_articles.category_id == 'sport')].iloc[7000:8000]\n",
    "e_train, e_test = df_articles[(df_articles.category_id == 'economie')].iloc[:4000],  df_articles[(df_articles.category_id == 'economie')].iloc[7000:8000]\n",
    "sc_train, sc_test = df_articles[(df_articles.category_id == 'arts-stars')].iloc[:4000],  df_articles[(df_articles.category_id == 'arts-stars')].iloc[7000:8000]\n",
    "h_train, h_test = df_articles[(df_articles.category_id == 'high-tech')].iloc[:4000],  df_articles[(df_articles.category_id == 'high-tech')].iloc[7000:8000]\n",
    "po_train, po_test = df_articles[(df_articles.category_id == 'politique')].iloc[:4000],  df_articles[(df_articles.category_id == 'politique')].iloc[7000:8000]\n",
    "a_train, a_test = train_test_split(df_autres, test_size=10000, train_size=4000, random_state=42)\n",
    "train_dataset = pd.concat([p_train, s_train, e_train, sc_train, h_train, po_train, a_train])[['body', 'category_id']]\n",
    "train_dataset['label'] = train_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)\n",
    "test_dataset = pd.concat([p_test, s_test, e_test, sc_test, h_test, po_test, a_test])[['body', 'category_id']]\n",
    "test_dataset['label'] = test_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gloabal parameters and tokenizer\n",
    "MAX_LEN_BODY = 512\n",
    "# MAX_LEN_TITLE = 64\n",
    "batch_size = 16\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creation of the test datasets ###\n",
    "# Creates list of texts and labels\n",
    "text = test_dataset['body'].to_list()\n",
    "# titles = test_dataset['title'].to_list()\n",
    "labels = test_dataset['label'].to_list()\n",
    "body_text = []\n",
    "body_labels = []\n",
    "body_id = []\n",
    "for i in range(len(text)):\n",
    "    parts = split_text_in_parts(text[i].split())\n",
    "    for part in parts:\n",
    "        if part != []:\n",
    "            body_text.append(' '.join(part))\n",
    "            body_labels.append(labels[i])\n",
    "            body_id.append(i)\n",
    "    \n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "# input_ids_titles  = tokenizer(titles, max_length=MAX_LEN_TITLE, padding='longest', truncation=True).input_ids\n",
    "input_ids_body  = tokenizer(body_text, max_length=MAX_LEN_BODY, padding='longest', truncation=True).input_ids\n",
    "\n",
    "# # Create attention masks for titles dataset\n",
    "# attention_masks_title = []\n",
    "# # Create a mask of 1s for each token followed by 0s for padding\n",
    "# for seq in input_ids_titles:\n",
    "#     seq_mask = [float(i!=1) for i in seq]  \n",
    "#     attention_masks_title.append(seq_mask)\n",
    "\n",
    "# Create attention masks for body dataset\n",
    "attention_masks_body = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_body:\n",
    "    seq_mask = [float(i!=1) for i in seq]  \n",
    "    attention_masks_body.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transfrom to tensor format\n",
    "# test_inputs_titles = torch.tensor(input_ids_titles)\n",
    "# test_labels_titles = torch.tensor(labels)\n",
    "# test_masks_titles = torch.tensor(attention_masks_title)\n",
    "\n",
    "# # create dataloader for body articles\n",
    "# test_data_title = TensorDataset(test_inputs_titles, test_masks_titles, test_labels_titles)\n",
    "# test_dataloader_title = DataLoader(test_data_title, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom to tensor format\n",
    "test_inputs_body = torch.tensor(input_ids_body)\n",
    "test_labels_body = torch.tensor(body_labels)\n",
    "test_masks_body = torch.tensor(attention_masks_body)\n",
    "\n",
    "# create dataloader for body articles\n",
    "test_data_body = TensorDataset(test_inputs_body, test_masks_body, test_labels_body)\n",
    "test_dataloader_body = DataLoader(test_data_body, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretained titles_model\n",
    "# model_title = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6).to(device)\n",
    "# model_title.load_state_dict(torch.load(\"../models/camembert_title_2.pt\"))\n",
    "\n",
    "# Load pretained body_model\n",
    "model_body = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6).to(device)\n",
    "model_body.load_state_dict(torch.load(\"../models/camembert_body_14K.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [00:23<00:00, 14.94it/s]\n",
      "100%|██████████| 443/443 [05:16<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# predictions with both models\n",
    "with torch.no_grad():\n",
    "        proba_titles = []\n",
    "        for batch in tqdm(test_dataloader_title):\n",
    "            t_data = batch[0].to(device)\n",
    "            t_mask = batch[1].to(device)\n",
    "            y = model_title(t_data,attention_mask=t_mask).logits.cpu().detach().numpy()\n",
    "            for proba in y:\n",
    "                proba_titles.append(proba)\n",
    "\n",
    "        proba_body = []\n",
    "        for batch in tqdm(test_dataloader_body):\n",
    "            t_data = batch[0].to(device)\n",
    "            t_mask = batch[1].to(device)\n",
    "            y = model_body(t_data,attention_mask=t_mask).logits.cpu().detach().numpy()\n",
    "            for proba in y:\n",
    "                proba_body.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500/5500 [00:00<00:00, 166708.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# regroup predictions for articles with long bodies (as long articles have been split before \n",
    "# the predictions because the model doesn't accept input with length above 512).\n",
    "# The importance of each prediction is proportionnal to the length of each part of the article.\n",
    "concat_proba_body = []\n",
    "c = 0\n",
    "for i in tqdm(range(len(proba_titles))):\n",
    "    proba = 0\n",
    "    n = 0\n",
    "    while c < len(body_id) and body_id[c] <= i:\n",
    "        ni = len(input_ids_body[c])\n",
    "        proba += ni * proba_body[c]\n",
    "        n += ni\n",
    "        c += 1\n",
    "    if n > 0:\n",
    "        concat_proba_body.append(proba / n)\n",
    "    else:\n",
    "        concat_proba_body.append(np.array([0]*6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final probability\n",
    "final_proba_body = np.array(concat_proba_body)\n",
    "final_proba_titles = np.array(proba_titles)\n",
    "\n",
    "final_proba = (final_proba_body + final_proba_titles) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "prob = softmax(final_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18976273, 0.38701603, 0.1062028 , 0.10523246, 0.16362542,\n",
       "       0.04816056])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[1357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract predicted class from proba\n",
    "y_pred = np.argmax(final_proba, axis=1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [np.argwhere(prob[i] > 0.2).flatten() for i in range(len(prob))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.116, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si = []\n",
    "for i in range(len(predictions)):\n",
    "    si.append(len(predictions[i]))\n",
    "sum(si) / len(predictions), np.max(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543636363636364"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(predictions)):\n",
    "    if labels[i] in predictions[i]:\n",
    "        c += 1\n",
    "c / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9190909090909091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred == labels) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[925,   3,  12,  19,  14,  27],\n",
       "       [  4, 982,   7,   0,   6,   1],\n",
       "       [ 35,  13, 838,   4,  49,  61],\n",
       "       [ 44,   2,   2, 432,  16,   4],\n",
       "       [ 10,   7,  20,  32, 917,  14],\n",
       "       [  8,   8,  16,   0,   7, 961]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupli_text = []\n",
    "dupli_labels = []\n",
    "true_lab = []\n",
    "dict_labels = {'planete': 0, 'sport': 1, 'economie': 2, 'sciences': 3, 'high-tech': 4, 'politique': 5}\n",
    "inv_dic = {v:k for (k, v) in dict_labels.items()}\n",
    "for i in range(len(predictions)):\n",
    "    if len(predictions[i]) > 1:\n",
    "        dupli_labels.append([inv_dic[k] for k in predictions[i]])\n",
    "        dupli_text.append(text[i])\n",
    "        true_lab.append(inv_dic[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['economie', 'high-tech'],\n",
       " 'high-tech',\n",
       " 'La scène remonte à juin 2015. Tranquillement installés devant leurs écrans d’ordinateur, deux chercheurs en cybersécurité, Charlie Miller et Chris Valasek, avaient réussi à prendre le contrôle d’une Jeep Cherokee conduite par un journaliste via une simple adresse IP. Le monde entier découvrait alors la vulnérabilité des voitures connectées face aux risques de cyberattaques. Depuis, d’autres modèles et d’autres marques de véhicules ont également subi ce même type d’attaque-test.Alors que l’avènement des voitures autonomes approche, la cybersécurité devient donc désormais un enjeu central des mobilités connectées. C’est sur ce thème que Lennig Pedron, présidente de l’ONG suisse Icon, interviendra jeudi à Rennes lors d’une conférence organisée dans le cadre de l’événement InOut.Les véhicules sont de plus en plus connectés et donc de plus en plus vulnérables aux cyberattaques. La menace est donc réelle ?Le danger existe bien sûr. A partir du moment où les véhicules sont connectés à internet, cela peut ouvrir des brèches pour des personnes mal intentionnées. On a eu la démonstration qu’on pouvait prendre à distance le contrôle d’une voiture. Les attaques menées ont d’ailleurs montré que même si l’environnement de la voiture est extrêmement moderne et intelligent, les technologies utilisées ne sont finalement pas si modernes.C’est assez inquiétant, non ?Effectivement car avec les mobilités, il y a clairement un danger de mort en cas de cyberattaque. Imaginez que vous êtes dans l’habitacle du véhicule avec vos enfants derrière et que vous ne contrôliez plus la voiture. C’est justement l’objectif de notre ONG à savoir informer et alerter le grand public sur les risques encourus.Les constructeurs ont-ils pris pleinement conscience de cette menace ?Ce n’est pas leur métier de base. Ils construisent des pièces mécaniques et non pas de l’informatique. On demande donc à l’industrie automobile de faire un grand pas en avant sur ces questions de cybersécurité. D’abord en étant plus transparent sur les moyens mis en œuvre pour assurer la sécurité des conducteurs. Il faut aussi que la sécurité soit prise en compte dès le début du projet, dans la phase recherche et développement. Ce qui n’est pas forcément le cas aujourd’hui.Pour quelles raisons ?On a l’impression que la cybersécurité les enquiquine. Car cela peut entraver la créativité et l’innovation. Il y a aussi la question du coût. On se retrouve donc avec des projets où les « airbags » de cybersécurité ne sont pensés qu’à la fin. C’est incohérent. Si ces questions étaient prises en compte dès le départ, il y a des chances que cela coûte moins cher sur le produit final.Comment le conducteur peut-il agir car c’est de sa propre sécurité dont il s’agit ?On ne doit pas laisser notre sécurité aux seules mains de constructeurs. Quand un client achète une voiture, il faut donc qu’il pose des questions. Les vendeurs nous parlent de la puissance du véhicule, du design mais quid de la sécurité ? Qu’est-ce qui se passe quand la technologie tombe en panne ? On est client, on doit donc avoir des réponses. Et à force, les constructeurs seront obligés d’intégrer ces questions dans leur argumentaire de vente.\\xa0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 450\n",
    "dupli_labels[l], true_lab[l], dupli_text[l]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
