{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 20 minutes articles\n",
    "df_articles1 = pd.read_parquet('../newspaper_part1.parquet')\n",
    "df_articles2 = pd.read_parquet('../newspaper_part2.parquet')\n",
    "df_articles = pd.concat([df_articles1, df_articles2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arts-stars</th>\n",
       "      <td>29765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economie</th>\n",
       "      <td>91356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high-tech</th>\n",
       "      <td>23072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planete</th>\n",
       "      <td>24854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politique</th>\n",
       "      <td>41204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>129275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title\n",
       "category_id        \n",
       "arts-stars    29765\n",
       "economie      91356\n",
       "high-tech     23072\n",
       "planete       24854\n",
       "politique     41204\n",
       "sport        129275"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = df_articles.sort_values(\"article_date\")\n",
    "df_autres = df_articles[(df_articles.category_id != 'planete') & (df_articles.category_id != 'sport') & (df_articles.category_id != 'economie') & (df_articles.category_id != 'arts-stars') & (df_articles.category_id != 'high-tech') & (df_articles.category_id != 'politique')]\n",
    "df_autres = df_autres[(df_autres.body != '') & (df_autres.title != '')]\n",
    "df_autres[\"category_id\"] = \"autres\"\n",
    "df_articles = df_articles[(df_articles.category_id == 'planete') | (df_articles.category_id == 'sport')| (df_articles.category_id == 'economie')| (df_articles.category_id == 'arts-stars')| (df_articles.category_id == 'high-tech')| (df_articles.category_id == 'politique')]\n",
    "df_articles = df_articles[(df_articles.body != '') & (df_articles.title != '')]\n",
    "df_articles.groupby(\"category_id\").count()[['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_in_parts(txt):\n",
    "    \"\"\"\n",
    "    Split articles in part of length 500 max to be compatible with camembert model.\n",
    "    \"\"\"\n",
    "    n = len(txt)\n",
    "    prev_cursor = 0\n",
    "    cursor = min(499, n-1)\n",
    "    parts = []\n",
    "    while prev_cursor < n-1:\n",
    "        while '.' not in txt[cursor] and cursor > prev_cursor:\n",
    "            cursor -= 1\n",
    "        if cursor == prev_cursor:\n",
    "            parts.append(txt[prev_cursor:min(prev_cursor+500, n)])\n",
    "            prev_cursor = min(prev_cursor+500, n)\n",
    "            cursor = min(prev_cursor+499, n-1)\n",
    "        else:\n",
    "            parts.append(txt[prev_cursor:cursor+1])\n",
    "            prev_cursor = cursor+1\n",
    "            cursor = prev_cursor+499\n",
    "            if cursor >= n-1 and prev_cursor < n-1:\n",
    "                parts.append(txt[prev_cursor:])\n",
    "                break\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {'planete': 0, 'sport': 1, 'economie': 2, 'arts-stars': 3, 'high-tech': 4, 'politique': 5, 'autres': 6}\n",
    "p_train, p_test = df_articles[(df_articles.category_id == 'planete')].iloc[:10000],  df_articles[(df_articles.category_id == 'planete')].iloc[10000:11000]\n",
    "s_train, s_test = df_articles[(df_articles.category_id == 'sport')].iloc[:10000],  df_articles[(df_articles.category_id == 'sport')].iloc[10000:11000]\n",
    "e_train, e_test = df_articles[(df_articles.category_id == 'economie')].iloc[:10000],  df_articles[(df_articles.category_id == 'economie')].iloc[10000:11000]\n",
    "sc_train, sc_test = df_articles[(df_articles.category_id == 'arts-stars')].iloc[:10000],  df_articles[(df_articles.category_id == 'arts-stars')].iloc[10000:11000]\n",
    "h_train, h_test = df_articles[(df_articles.category_id == 'high-tech')].iloc[:10000],  df_articles[(df_articles.category_id == 'high-tech')].iloc[10000:11000]\n",
    "po_train, po_test = df_articles[(df_articles.category_id == 'politique')].iloc[:10000],  df_articles[(df_articles.category_id == 'politique')].iloc[10000:11000]\n",
    "a_train, a_test = train_test_split(df_autres, test_size=2000, train_size=20000, random_state=42)\n",
    "train_dataset = pd.concat([p_train, s_train, e_train, sc_train, h_train, po_train, a_train])[['body', 'category_id']]\n",
    "train_dataset['label'] = train_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)\n",
    "test_dataset = pd.concat([p_test, s_test, e_test, sc_test, h_test, po_test, a_test])[['body', 'category_id']]\n",
    "test_dataset['label'] = test_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gloabal parameters and tokenizer\n",
    "MAX_LEN = 512\n",
    "batch_size = 16\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creation of the test dataset ###\n",
    "# Creates list of texts and labels\n",
    "text = test_dataset['body'].to_list()\n",
    "labels = test_dataset['label'].to_list()\n",
    "body_text = []\n",
    "body_labels = []\n",
    "for i in range(len(text)):\n",
    "    parts = split_text_in_parts(text[i].split())\n",
    "    for part in parts:\n",
    "        if part != []:\n",
    "            body_text.append(' '.join(part))\n",
    "            body_labels.append(labels[i])\n",
    "    \n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = tokenizer(body_text, max_length=MAX_LEN, padding='longest', truncation=True).input_ids\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i!=1) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom to tensor format\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(body_labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# create dataloader\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:01<00:00, 45356.09it/s]\n",
      "100%|██████████| 102744/102744 [00:03<00:00, 30549.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creates list of texts and labels\n",
    "text = train_dataset['body'].to_list()\n",
    "labels = train_dataset['label'].to_list()\n",
    "body_text = []\n",
    "body_labels = []\n",
    "for i in tqdm(range(len(text))):\n",
    "    parts = split_text_in_parts(text[i].split())\n",
    "    for part in parts:\n",
    "        if part != []:\n",
    "            body_text.append(' '.join(part))\n",
    "            body_labels.append(labels[i])\n",
    "    \n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = tokenizer(body_text, max_length=MAX_LEN, padding='longest', truncation=True).input_ids\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in tqdm(input_ids):\n",
    "    seq_mask = [float(i!=1) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom to tensor format\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "train_labels = torch.tensor(body_labels)\n",
    "train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# create dataloader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretained model\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute accuracy\n",
    "\n",
    "def compute_accuracy(test_dataloader, model):\n",
    "    total_true = 0\n",
    "    total_size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            t_data = batch[0].to(device)\n",
    "            t_mask = batch[1].to(device)\n",
    "            y = model(t_data,attention_mask=t_mask).logits\n",
    "            result = torch.argmax(y, dim=-1).cpu().detach().numpy()\n",
    "            nb_true, size = np.sum(result == np.array(batch[2])), len(result)\n",
    "            total_true += nb_true\n",
    "            total_size += size\n",
    "            t_data.cpu()\n",
    "            t_mask.cpu()\n",
    "            del batch\n",
    "            del y\n",
    "            del t_data\n",
    "            del t_mask\n",
    "            gc.collect()\n",
    "        \n",
    "    accuracy = total_true / total_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract parameters to optimize\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "opt = torch.optim.Adam(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n",
    "\n",
    "\n",
    "def train(classifier, data, epochs=20, log_interval=50):\n",
    "    best_acc = 0\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for idx, batch in enumerate(data):\n",
    "            print(idx)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = classifier(b_input_ids, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            ## del GPU variables to keep free memory\n",
    "            # loss.detach().cpu()\n",
    "            # b_input_ids.cpu()\n",
    "            # b_input_mask.cpu()\n",
    "            # b_labels.cpu()\n",
    "            del(loss)\n",
    "            del(batch)\n",
    "            del(b_input_ids, b_input_mask, b_labels)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
    "                    \"loss {:5.5f}\".format(\n",
    "                        epoch, idx, len(data), cur_loss,\n",
    "                    )\n",
    "                )\n",
    "                losses.append(cur_loss)\n",
    "                total_loss = 0\n",
    "        accuracy = compute_accuracy(test_dataloader, model)\n",
    "        print(\"Test accuracy : {:1.3f}\".format(accuracy))\n",
    "        # Save model if better\n",
    "        if accuracy > best_acc:\n",
    "            torch.save(model.state_dict(), '../models/camembert_body_full.pt')\n",
    "            best_acc = accuracy\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, train_dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/camembert_body_2f.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
