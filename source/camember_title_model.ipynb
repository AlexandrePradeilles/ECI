{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CamembertConfig, CamembertModel, AutoTokenizer, CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 20 minutes articles\n",
    "df_articles = pd.read_parquet('../newspaper_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arts-stars</th>\n",
       "      <td>28294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economie</th>\n",
       "      <td>41992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high-tech</th>\n",
       "      <td>13565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planete</th>\n",
       "      <td>11055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politique</th>\n",
       "      <td>16601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>71847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title\n",
       "category_id       \n",
       "arts-stars   28294\n",
       "economie     41992\n",
       "high-tech    13565\n",
       "planete      11055\n",
       "politique    16601\n",
       "sport        71847"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = df_articles.sort_values(\"article_date\")\n",
    "df_autres = df_articles[(df_articles.category_id != 'planete') & (df_articles.category_id != 'sport') & (df_articles.category_id != 'economie') & (df_articles.category_id != 'arts-stars') & (df_articles.category_id != 'high-tech') & (df_articles.category_id != 'politique')]\n",
    "df_autres = df_autres[(df_autres.body != '') & (df_autres.title != '')]\n",
    "df_autres[\"category_id\"] = \"autres\"\n",
    "df_articles = df_articles[(df_articles.category_id == 'planete') | (df_articles.category_id == 'sport')| (df_articles.category_id == 'economie')| (df_articles.category_id == 'arts-stars')| (df_articles.category_id == 'high-tech')| (df_articles.category_id == 'politique')]\n",
    "df_articles = df_articles[(df_articles.body != '') & (df_articles.title != '')]\n",
    "df_articles.groupby(\"category_id\").count()[['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {'planete': 0, 'sport': 1, 'economie': 2, 'arts-stars': 3, 'high-tech': 4, 'politique': 5, 'autres': 6}\n",
    "p_train, p_test = df_articles[(df_articles.category_id == 'planete')].iloc[:7000],  df_articles[(df_articles.category_id == 'planete')].iloc[7000:8000]\n",
    "s_train, s_test = df_articles[(df_articles.category_id == 'sport')].iloc[:7000],  df_articles[(df_articles.category_id == 'sport')].iloc[7000:8000]\n",
    "e_train, e_test = df_articles[(df_articles.category_id == 'economie')].iloc[:7000],  df_articles[(df_articles.category_id == 'economie')].iloc[7000:8000]\n",
    "sc_train, sc_test = df_articles[(df_articles.category_id == 'arts-stars')].iloc[:7000],  df_articles[(df_articles.category_id == 'arts-stars')].iloc[7000:8000]\n",
    "h_train, h_test = df_articles[(df_articles.category_id == 'high-tech')].iloc[:7000],  df_articles[(df_articles.category_id == 'high-tech')].iloc[7000:8000]\n",
    "po_train, po_test = df_articles[(df_articles.category_id == 'politique')].iloc[:7000],  df_articles[(df_articles.category_id == 'politique')].iloc[7000:8000]\n",
    "a_train, a_test = train_test_split(df_autres, test_size=14000, train_size=5000, random_state=42)\n",
    "train_dataset = pd.concat([p_train, s_train, e_train, sc_train, h_train, po_train, a_train])[['title', 'category_id']]\n",
    "train_dataset['label'] = train_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)\n",
    "test_dataset = pd.concat([p_test, s_test, e_test, sc_test, h_test, po_test, a_test])[['title', 'category_id']]\n",
    "test_dataset['label'] = test_dataset.apply(lambda x: dict_labels[x['category_id']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gloabal parameters and tokenizer\n",
    "MAX_LEN = 64\n",
    "batch_size = 32\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creation of the test dataset ###\n",
    "# Creates list of texts and labels\n",
    "text = test_dataset['title'].to_list()\n",
    "labels = test_dataset['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = tokenizer(text, max_length=MAX_LEN, padding='longest', truncation=True).input_ids\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i!=1) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom to tensor format\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# create dataloader\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates list of texts and labels\n",
    "text = train_dataset['title'].to_list()\n",
    "labels = train_dataset['label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = tokenizer(text, max_length=MAX_LEN, padding='longest', truncation=True).input_ids\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i!=1) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom to tensor format\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "train_labels = torch.tensor(labels)\n",
    "train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# create dataloader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretained model\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute accuracy\n",
    "\n",
    "def compute_accuracy(test_dataloader, model):\n",
    "    total_true = 0\n",
    "    total_size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            t_data = batch[0].to(device)\n",
    "            t_mask = batch[1].to(device)\n",
    "            y = model(t_data,attention_mask=t_mask).logits\n",
    "            result = torch.argmax(y, dim=-1).cpu().detach().numpy()\n",
    "            nb_true, size = np.sum(result == np.array(batch[2])), len(result)\n",
    "            total_true += nb_true\n",
    "            total_size += size\n",
    "            t_data.cpu()\n",
    "            t_mask.cpu()\n",
    "            del batch\n",
    "            del y\n",
    "            del t_data\n",
    "            del t_mask\n",
    "            gc.collect()\n",
    "        \n",
    "    accuracy = total_true / total_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract parameters to optimize\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "opt = torch.optim.Adam(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n",
    "\n",
    "\n",
    "def train(classifier, data, epochs=5, log_interval=50):\n",
    "    best_acc = 0\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "            # Train model on batch\n",
    "            opt.zero_grad()\n",
    "            y_pred = classifier(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = y_pred[0]\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            # delete variables to keep memory on gpu\n",
    "            loss.detach().cpu()\n",
    "            b_input_ids.cpu()\n",
    "            b_input_mask.cpu()\n",
    "            b_labels.cpu()\n",
    "            del(y_pred)\n",
    "            del(loss)\n",
    "            del(batch)\n",
    "            del(b_input_ids, b_input_mask, b_labels)\n",
    "            gc.collect()\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
    "                    \"loss {:5.5f}\".format(\n",
    "                        epoch, idx, len(data), cur_loss,\n",
    "                    )\n",
    "                )\n",
    "                losses.append(cur_loss)\n",
    "                total_loss = 0\n",
    "        accuracy = compute_accuracy(test_dataloader, model)\n",
    "        print(\"Test accuracy : {:1.3f}\".format(accuracy))\n",
    "        # Save model if better\n",
    "        if accuracy > best_acc:\n",
    "            torch.save(model.state_dict(), '../models/camembert_title_14K.pt')\n",
    "            best_acc = accuracy\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    50/ 1469 steps | loss 1.81394\n",
      "| epoch   0 |   100/ 1469 steps | loss 1.25588\n",
      "| epoch   0 |   150/ 1469 steps | loss 0.89543\n",
      "| epoch   0 |   200/ 1469 steps | loss 0.78953\n",
      "| epoch   0 |   250/ 1469 steps | loss 0.67403\n",
      "| epoch   0 |   300/ 1469 steps | loss 0.62081\n",
      "| epoch   0 |   350/ 1469 steps | loss 0.58372\n",
      "| epoch   0 |   400/ 1469 steps | loss 0.56182\n",
      "| epoch   0 |   450/ 1469 steps | loss 0.47667\n",
      "| epoch   0 |   500/ 1469 steps | loss 0.50559\n",
      "| epoch   0 |   550/ 1469 steps | loss 0.49567\n",
      "| epoch   0 |   600/ 1469 steps | loss 0.44547\n",
      "| epoch   0 |   650/ 1469 steps | loss 0.45166\n",
      "| epoch   0 |   700/ 1469 steps | loss 0.46312\n",
      "| epoch   0 |   750/ 1469 steps | loss 0.49224\n",
      "| epoch   0 |   800/ 1469 steps | loss 0.45498\n",
      "| epoch   0 |   850/ 1469 steps | loss 0.46739\n",
      "| epoch   0 |   900/ 1469 steps | loss 0.42009\n",
      "| epoch   0 |   950/ 1469 steps | loss 0.47173\n",
      "| epoch   0 |  1000/ 1469 steps | loss 0.44227\n",
      "| epoch   0 |  1050/ 1469 steps | loss 0.45995\n",
      "| epoch   0 |  1100/ 1469 steps | loss 0.43144\n",
      "| epoch   0 |  1150/ 1469 steps | loss 0.43176\n",
      "| epoch   0 |  1200/ 1469 steps | loss 0.41668\n",
      "| epoch   0 |  1250/ 1469 steps | loss 0.43236\n",
      "| epoch   0 |  1300/ 1469 steps | loss 0.40167\n",
      "| epoch   0 |  1350/ 1469 steps | loss 0.40738\n",
      "| epoch   0 |  1400/ 1469 steps | loss 0.37155\n",
      "| epoch   0 |  1450/ 1469 steps | loss 0.40722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [02:52<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.671\n",
      "| epoch   1 |    50/ 1469 steps | loss 0.33204\n",
      "| epoch   1 |   100/ 1469 steps | loss 0.24871\n",
      "| epoch   1 |   150/ 1469 steps | loss 0.26962\n",
      "| epoch   1 |   200/ 1469 steps | loss 0.29491\n",
      "| epoch   1 |   250/ 1469 steps | loss 0.25126\n",
      "| epoch   1 |   300/ 1469 steps | loss 0.28984\n",
      "| epoch   1 |   350/ 1469 steps | loss 0.29596\n",
      "| epoch   1 |   400/ 1469 steps | loss 0.27086\n",
      "| epoch   1 |   450/ 1469 steps | loss 0.27196\n",
      "| epoch   1 |   500/ 1469 steps | loss 0.27721\n",
      "| epoch   1 |   550/ 1469 steps | loss 0.26761\n",
      "| epoch   1 |   600/ 1469 steps | loss 0.25930\n",
      "| epoch   1 |   650/ 1469 steps | loss 0.26737\n",
      "| epoch   1 |   700/ 1469 steps | loss 0.29556\n",
      "| epoch   1 |   750/ 1469 steps | loss 0.29903\n",
      "| epoch   1 |   800/ 1469 steps | loss 0.27771\n",
      "| epoch   1 |   850/ 1469 steps | loss 0.29790\n",
      "| epoch   1 |   900/ 1469 steps | loss 0.27438\n",
      "| epoch   1 |   950/ 1469 steps | loss 0.25406\n",
      "| epoch   1 |  1000/ 1469 steps | loss 0.22844\n",
      "| epoch   1 |  1050/ 1469 steps | loss 0.25966\n",
      "| epoch   1 |  1100/ 1469 steps | loss 0.27998\n",
      "| epoch   1 |  1150/ 1469 steps | loss 0.27263\n",
      "| epoch   1 |  1200/ 1469 steps | loss 0.25561\n",
      "| epoch   1 |  1250/ 1469 steps | loss 0.28083\n",
      "| epoch   1 |  1300/ 1469 steps | loss 0.26581\n",
      "| epoch   1 |  1350/ 1469 steps | loss 0.27109\n",
      "| epoch   1 |  1400/ 1469 steps | loss 0.27044\n",
      "| epoch   1 |  1450/ 1469 steps | loss 0.25956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [02:49<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.704\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/camembert_title_7K.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
